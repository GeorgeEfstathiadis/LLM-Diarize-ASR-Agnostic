{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils.data import extract_text_and_spk, format_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data to only include the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing ids\n",
    "with open('./data/fisher.txt', 'r') as file:\n",
    "    fids = file.read().split('\\n')\n",
    "    fids = [x for x in fids if x]\n",
    "\n",
    "# load the json file with the processed data\n",
    "with open('./data/processed_data.json',\n",
    "            'r') as file:\n",
    "        res = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the utterances that are in the testing set\n",
    "res2 = {'utterances': [x for x in res['utterances'] if x['utterance_id'] not in fids]}\n",
    "\n",
    "with open('./data/processed_data_train.json', 'w') as file:\n",
    "    json.dump(res2, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the data into prompts and completions. We will use the `train_data_prep.py` script to do this. The script will take the input data and output prompts and completions in the format required by the model. The script will also add the speaker information to the prompts and completions from hyp_spk and hyp_spk_oracle fields in the input data.\n",
    "\n",
    "```shell\n",
    "python3 train_data_prep.py \\\n",
    "--input=\"./data/processed_data_train.json\" \\\n",
    "--output=\"./data/prompts_train.jsonl\" \\\n",
    "--output_type=jsonl \\\n",
    "--emit_input_length=2500 \\\n",
    "--emit_target_length=2500 \\\n",
    "--prompt_suffix=\"\" \\\n",
    "--completion_suffix=\"\" \\\n",
    "--input_feature_key=\"prompt\" \\\n",
    "--output_feature_key=\"completion\" \\\n",
    "--text_field=\"hyp_text\" \\\n",
    "--input_speaker_field=\"hyp_spk\" \\\n",
    "--target_speaker_field=\"hyp_spk_oracle\" \\\n",
    "--speaker_prefix=\"<spk:\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train_data\n",
    "## file is a jsonl with a dictionary in each line\n",
    "with open('./data/prompts_train.jsonl', 'r') as file:\n",
    "    train_data = [json.loads(x) for x in file]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are needed to preprocess the data for training:\n",
    "\n",
    "1. Remove prompts and completions that have a repeated word/phtrase issue.\n",
    "2. Convert to the instruction format required by the model.\n",
    "3. Tokenize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop in utterances - check if same word/phrase (up to 3 words) is repeated 10 times consecutively\n",
    "repeated_words = []\n",
    "train_data2 = []\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    x = train_data[i]\n",
    "    prompt = x['prompt']\n",
    "    words, _ = extract_text_and_spk(prompt)\n",
    "    words = words.split()\n",
    "    for k in range(len(words)-10):\n",
    "        if len(set(words[k:k+10])) == 1:\n",
    "            repeated_words.append(x['utterance_id'])\n",
    "            break\n",
    "    for k in range(len(words)-20):\n",
    "        if len(set(words[k:k+20])) == 2:\n",
    "            repeated_words.append(x['utterance_id'])\n",
    "            break\n",
    "    for k in range(len(words)-30):\n",
    "        if len(set(words[k:k+30])) == 3:\n",
    "            repeated_words.append(x['utterance_id'])\n",
    "            break\n",
    "    if x['utterance_id'] not in repeated_words:\n",
    "       train_data2.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to prompt and completion\n",
    "dataset_jsonl = {\n",
    "    \"instruction\": [f\"In the speaker diarization transcript below, some words are potentially misplaced. Please correct those words and move them to the right speaker. Directly show the corrected transcript without explaining what changes were made or why you made those changes.:\\n\\n{x['prompt']}\" for x in train_data2],\n",
    "    \"response\": [x['completion'] for x in train_data2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "\n",
    "final_data = pd.DataFrame(dataset_jsonl)\n",
    "dataset = Dataset.from_pandas(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_data(sample)}{tokenizer.eos_token}\"\n",
    "\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "lm_dataset.save_to_disk('./train')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
