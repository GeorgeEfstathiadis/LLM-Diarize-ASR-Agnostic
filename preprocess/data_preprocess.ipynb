{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from diarization_utils.utils import transcript_preserving_speaker_transfer\n",
    "from utils.data import true_labels, aws_labels, create_diarized_text\n",
    "from utils.metrics import calculate_swer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file paths - this is a dataframe with the following columns:\n",
    "# utt_id, file_path_original, file_path_asr\n",
    "# utt_id is the file name\n",
    "# file_path_original is the path to the original transcription file\n",
    "# file_path_asr is the path to the ASR transcribed file\n",
    "file_paths_df = pd.read_csv('./data/file_paths.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a for-loop across all the files in the dataframe, where for each file we:\n",
    "\n",
    "1. Extract words, speaker labels and timings from the reference transcript.\n",
    "2. Use these timings to deduce the part of the audio file that corresponds to the reference transcript.\n",
    "3. Extract the words, speaker labels and timings from the ASR output that correspond to the audio segment.\n",
    "4. Skip the file if the ASR output has only one speaker.\n",
    "5. Deduce if the speaker labels in the ASR output match the speaker labels in the reference transcript or should be swapped.\n",
    "6. Transfer the speaker labels from the reference transcript to the ASR output using TPST.\n",
    "7. Store the results in a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "   'utterances': []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(file_paths_df))):\n",
    "    utterance_id = file_paths_df.iloc[i]['utt_id']\n",
    "    original_file = file_paths_df.iloc[i]['file_path_original']\n",
    "    aws_file = file_paths_df.iloc[i]['file_path_asr']\n",
    "\n",
    "    ref_labels, ref_words, ref_times = true_labels(original_file)\n",
    "    start_time, end_time = ref_times[0][0], ref_times[-1][1]\n",
    "\n",
    "    hyp_words, hyp_labels1 = aws_labels(aws_file, start_time, end_time)\n",
    "\n",
    "    # check for single speaker\n",
    "    if len(set(hyp_labels1)) == 1:\n",
    "        continue\n",
    "\n",
    "    hyp_labels2 = ['1' if x == '2' else '2' for x in hyp_labels1]\n",
    "\n",
    "    # Calculate the SWER to determine which speaker diarization is better\n",
    "    # i.e. which speaker labelling matches the reference speaker labelling\n",
    "    swer1 = calculate_swer(ref_words, ref_labels, hyp_words, hyp_labels1)\n",
    "    swer2 = calculate_swer(ref_words, ref_labels, hyp_words, hyp_labels2)\n",
    "    if swer1 < swer2:\n",
    "        hyp_labels = hyp_labels1\n",
    "    else:\n",
    "        hyp_labels = hyp_labels2\n",
    "\n",
    "    # Transfer the speaker labels from the ASR to the original transcription\n",
    "    hyp_spk_oracle = transcript_preserving_speaker_transfer(\n",
    "        \" \".join(ref_words), \" \".join(ref_labels), \" \".join(hyp_words), \" \".join(hyp_labels)\n",
    "    )\n",
    "\n",
    "    res['utterances'].append({\n",
    "        'utterance_id': utterance_id,\n",
    "        'ref_text': \" \".join(ref_words),\n",
    "        'ref_spk': \" \".join(ref_labels),\n",
    "        'ref_diarized_text': create_diarized_text(ref_words, ref_labels),\n",
    "        'hyp_text': \" \".join(hyp_words),\n",
    "        'hyp_spk': \" \".join(hyp_labels),\n",
    "        'hyp_diarized_text': create_diarized_text(hyp_words, hyp_labels),\n",
    "        'hyp_spk_oracle': hyp_spk_oracle,\n",
    "        'hyp_diarized_text_oracle': create_diarized_text(hyp_words, hyp_spk_oracle.split(' ')),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed_data.json', 'w') as file:\n",
    "        json.dump(res, file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
