{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import meeteval\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from diarization_utils.utils import transcript_preserving_speaker_transfer\n",
    "from utils.metrics import calculate_cpwer, calculate_swer, preprocess_str\n",
    "from utils.data import extract_text_and_spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/full_test.json', 'r') as file:\n",
    "    data = [json.loads(x) for x in file][0]['utterances']\n",
    "    data2 = {x['utterance_id']: x for x in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove utterances with 10 consecutive words/phrases (max 3 words)\n",
    "consecutive_words = []\n",
    "for i in range(len(data)):\n",
    "    words = data[i]['hyp_text'].split()\n",
    "    for j in range(len(words)-10):\n",
    "        if len(set(words[j:j+10])) == 1:\n",
    "            consecutive_words.append(data[i]['utterance_id'])\n",
    "            break\n",
    "    for j in range(len(words)-20):\n",
    "        if len(set(words[j:j+20])) == 2:\n",
    "            consecutive_words.append(data[i]['utterance_id'])\n",
    "            break\n",
    "    for j in range(len(words)-30):\n",
    "        if len(set(words[j:j+30])) == 3:\n",
    "            consecutive_words.append(data[i]['utterance_id'])\n",
    "            break\n",
    "data = [x for x in data if x['utterance_id'] not in consecutive_words]\n",
    "data2 = {x['utterance_id']: x for x in data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_wer = []\n",
    "\n",
    "baseline_cpwer = []\n",
    "baseline_swer = []\n",
    "\n",
    "for i in tqdm(range(len(data2.keys()))):\n",
    "    idx = list(data2.keys())[i]\n",
    "\n",
    "    words = data2[idx]['hyp_text'].split()\n",
    "\n",
    "    speakers_ref = data2[idx]['hyp_spk_oracle']\n",
    "    speakers_input = data2[idx]['hyp_spk']\n",
    "\n",
    "    base_cpwer = calculate_cpwer(\n",
    "        data2[idx]['hyp_text'], data2[idx]['hyp_spk'], data2[idx]['ref_text'], data2[idx]['ref_spk']\n",
    "    )\n",
    "    baseline_cpwer.append(base_cpwer)\n",
    "\n",
    "    base_swer = calculate_swer(\n",
    "        data2[idx]['hyp_text'], data2[idx]['hyp_spk'], data2[idx]['ref_text'], data2[idx]['ref_spk']\n",
    "    )\n",
    "    baseline_swer.append(base_swer)\n",
    "\n",
    "    # calculate WER\n",
    "    wer = meeteval.wer.wer.siso.siso_word_error_rate(\n",
    "        reference=data2[idx]['ref_text'],\n",
    "        hypothesis=data2[idx]['hyp_text']\n",
    "    ).error_rate * 100\n",
    "\n",
    "    baseline_wer.append(wer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(baseline_wer), np.std(baseline_wer)\n",
    "np.mean(baseline_cpwer), np.std(baseline_cpwer)\n",
    "np.mean(baseline_swer), np.std(baseline_swer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './results/model_predictions.json'\n",
    "\n",
    "with open(filepath, 'r') as file:\n",
    "    finetuned = json.load(file)\n",
    "\n",
    "keys = sorted([k for k in finetuned.keys()])\n",
    "for i in sorted(keys):\n",
    "    utt_id = i.split('_seg')[0]\n",
    "    data2[utt_id]['completions_llm'].append(finetuned[i].split('### Answer\\n\\n')[1].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten completions_llm\n",
    "unfinalized_outputs = []\n",
    "for key in data2.keys():\n",
    "    if len(data2[key]['completions_llm']) < len(data2[key]['prompts_unprocessed']):\n",
    "        unfinalized_outputs.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3 is the finalized data\n",
    "data3 = {x: data2[x] for x in data2.keys() if x not in unfinalized_outputs}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cpwer = []\n",
    "results_swer = []\n",
    "\n",
    "for i in tqdm(range(len(data3))):\n",
    "    key = list(data3.keys())[i]\n",
    "\n",
    "    speakers_ref = data3[key]['hyp_spk_oracle']\n",
    "    speakers_input = data3[key]['hyp_spk']\n",
    "    words_input = data3[key]['hyp_text']\n",
    "    words_trans = data3[key]['ref_text']\n",
    "    speakers_trans = data3[key]['ref_spk']\n",
    "\n",
    "    speakers_pred = \"\"\n",
    "\n",
    "    for j in range(len(data3[key]['completions_llm'])):\n",
    "        input = data3[key]['prompts_unprocessed'][j]\n",
    "        output = data3[key]['completions_llm'][j]\n",
    "                \n",
    "        input = preprocess_str(input)\n",
    "        output = preprocess_str(output)\n",
    "\n",
    "        # extract text and speaker\n",
    "        words_in, speakers_in = extract_text_and_spk(input)\n",
    "        words_out, speakers_out = extract_text_and_spk(output)\n",
    "\n",
    "        # transfer speakers from out to ref\n",
    "        speakers_out2 = transcript_preserving_speaker_transfer(words_out, speakers_out, words_in, speakers_in)\n",
    "        assert len(speakers_out2.split()) == len(speakers_in.split())\n",
    "\n",
    "        speakers_pred += \" \" + speakers_out2\n",
    "\n",
    "    speakers_pred = speakers_pred[1:]\n",
    "    assert len(speakers_pred.split()) == len(speakers_input.split())\n",
    "    assert len(words_input.split()) == len(speakers_input.split())\n",
    "    assert len(words_input.split()) == len(speakers_ref.split())\n",
    "\n",
    "    result_cpwer = calculate_cpwer(\n",
    "        words_input, speakers_pred, words_trans, speakers_trans\n",
    "    )\n",
    "    result_swer = calculate_swer(\n",
    "        words_input, speakers_pred, words_trans, speakers_trans\n",
    "    )\n",
    "\n",
    "    results_cpwer.append(result_cpwer)\n",
    "    results_swer.append(result_swer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results_cpwer), np.std(results_cpwer)\n",
    "np.mean(baseline_cpwer), np.std(baseline_cpwer)\n",
    "\n",
    "np.mean(results_swer), np.std(results_swer)\n",
    "np.mean(baseline_swer), np.std(baseline_swer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
